2016-11-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* hypo: new hypothesis: large learning rates are not bad because
	you overstep they are bad because you're stepping in a noisy
	direction.  Even when we take good Newton steps every minibatch,
	the overall performance gets worse.

	We can try to write a version of SGD with real gradients of mini
	batches that take little steps every time to confirm that it
	doesn't work.  Then look at adam to see rates adjusted dynamically
	and apply the idea to gfo.  Adam etc. build a diagonal Hessian
	instead of a spherical Hessian, I think.

2016-11-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* mnist.jl (gloss): large steps that do well on a minibatch hurt
	the overall performance.  Small lr (0.05) necessary for continued
	progress.  hpred stable around 0.002 in later epochs.  hpred and w
	both like to change slowly.  Next put some momentum on gpred.
	Best results so far can get to 0.38 loss in 100 epochs.

2016-11-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* mnist.jl (gloss): hpred keeps decreasing even though if we are
	doing worse (delta = dfpred-dfgold < 0) it should increase
	(i.e. lr=1/hpred should decrease).  While debugging that I started
	getting NaNs.

2016-11-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* gradvar: Look at the variance in gradient across different
	minibatches for different minibatch sizes as well as across
	different steps of the SGD algorithm.

	* l1reg: L1 regularization should never change the sign of a
	coefficient.  Implementing this may allow us to use higher L1
	coefficients.

2016-10-31  Deniz Yuret  <dyuret@ku.edu.tr>

	* mnist: predicting gradient at 0.  linreg > logreg > perceptron >
	spsa in terms of convergence speed.  l2 is useless, l1 gives a
	slight advantage.  explore active learning next.


